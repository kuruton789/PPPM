{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"pppm-mlm.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["ref: https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/323095"],"metadata":{"id":"YlzE11qsnO6P"}},{"cell_type":"code","source":["import os\n","import sys\n","import torch\n","\n","class Config:\n","    AUTHOR = \"kuruton\"\n","\n","    NAME = \"USP-\" + \"MLM-deberta-v3-large-kf\"\n","\n","    MODEL_PATH = \"microsoft/deberta-v3-large\"\n","    DATASET_PATH = [\n","        \"yasufuminakama/cpc-data\"\n","    ]\n","\n","    COMPETITION = \"us-patent-phrase-to-phrase-matching\"\n","    COLAB_PATH = \"/content/drive/Shareddrives/USPatent\" \n","    DRIVE_PATH = os.path.join(COLAB_PATH, AUTHOR)\n","\n","    api_path = \"/content/drive/MyDrive/kaggle.json\"\n","\n","    seed = 42\n","    num_fold = 4\n","    trn_fold = [0, 1, 2, 3]\n","    batch_size = 32\n","    n_epochs = 10\n","    max_len = 256\n","\n","    weight_decay = 2e-5\n","    beta = (0.9, 0.98)\n","    lr = 2e-5\n","    num_warmup_steps_rate = 0.01\n","    clip_grad_norm = None\n","    gradient_accumulation_steps = 1\n","    num_eval = 1\n","    WEB_HOOK_URL = \"https://hooks.slack.com/services/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\"\n","\n","    debug = False\n","\n","    upload_from_colab = False"],"metadata":{"id":"7UbOxzknoRCl","executionInfo":{"status":"ok","timestamp":1652517328710,"user_tz":-540,"elapsed":3877,"user":{"displayName":"クルトン","userId":"17231302919530674825"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# ========================================\n","# Library\n","# ========================================\n","import os\n","import gc\n","import re\n","import sys\n","import json\n","import time\n","import shutil\n","import joblib\n","import random\n","import requests\n","import warnings\n","warnings.filterwarnings('ignore')\n","from ast import literal_eval\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import scipy \n","import itertools\n","from pathlib import Path\n","from glob import glob\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import (\n","    StratifiedKFold, \n","    KFold, \n","    GroupKFold\n",")\n","from sklearn.metrics import (\n","    accuracy_score, \n","    f1_score,\n","    roc_auc_score,\n",")\n","\n","from google.colab import drive\n","if not os.path.isdir('/content/drive'):\n","    drive.mount('/content/drive') "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58fNRii-skeo","executionInfo":{"status":"ok","timestamp":1652517357853,"user_tz":-540,"elapsed":21158,"user":{"displayName":"クルトン","userId":"17231302919530674825"}},"outputId":"2c68124f-fe22-4e5a-ad9a-e8dffbe721c6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# torch downgrade\n","! pip install -q torch==1.10.0\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch.cuda.amp import autocast, GradScaler"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6iPBdcHshpT","executionInfo":{"status":"ok","timestamp":1652517476833,"user_tz":-540,"elapsed":112529,"user":{"displayName":"クルトン","userId":"17231302919530674825"}},"outputId":"590121f3-2f7e-422b-f5ff-0aaaa819ad5d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:41tcmalloc: large alloc 1147494400 bytes == 0x649f8000 @  0x7f3e432c3615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████████████████| 881.9 MB 19 kB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["def setup(cfg):\n","    cfg.COLAB = 'google.colab' in sys.modules\n","    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    if cfg.COLAB:\n","        print('This environment is Google Colab')\n","\n","        # mount\n","        from google.colab import drive\n","        if not os.path.isdir('/content/drive'):\n","            drive.mount('/content/drive') \n","\n","        # pip install\n","        ! pip install -q torch==1.10.0\n","        ! pip install -q transformers\n","        ! pip install -q sentencepiece\n","\n","        # use kaggle api (need kaggle token)\n","        f = open(cfg.api_path, 'r')\n","        json_data = json.load(f) \n","        os.environ['KAGGLE_USERNAME'] = json_data['username']\n","        os.environ['KAGGLE_KEY'] = json_data['key']\n","\n","        # set dirs\n","        cfg.DRIVE = cfg.DRIVE_PATH\n","        cfg.EXP = (cfg.NAME if cfg.NAME is not None \n","            else requests.get('http://172.28.0.2:9000/api/sessions').json()[0]['name'][:-6]\n","        )\n","        cfg.INPUT = os.path.join(cfg.DRIVE, 'Input')\n","        cfg.OUTPUT = os.path.join(cfg.DRIVE, 'Output')\n","        cfg.SUBMISSION = os.path.join(cfg.DRIVE, 'Submission')\n","        cfg.DATASET = os.path.join(cfg.DRIVE, 'Dataset')\n","\n","        cfg.OUTPUT_EXP = os.path.join(cfg.OUTPUT, cfg.EXP) \n","        cfg.EXP_MODEL = os.path.join(cfg.OUTPUT_EXP, 'model')\n","        cfg.EXP_FIG = os.path.join(cfg.OUTPUT_EXP, 'fig')\n","        cfg.EXP_PREDS = os.path.join(cfg.OUTPUT_EXP, 'preds')\n","\n","        # make dirs\n","        for d in [cfg.INPUT, cfg.SUBMISSION, cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n","            os.makedirs(d, exist_ok=True)\n","        \n","        if not os.path.isfile(os.path.join(cfg.INPUT, 'train.csv')):\n","            # load dataset\n","            ! pip install --upgrade --force-reinstall --no-deps kaggle\n","            ! kaggle competitions download -c $cfg.COMPETITION -p $cfg.INPUT\n","            filepath = os.path.join(cfg.INPUT,cfg.COMPETITION+'.zip')\n","            ! unzip -d $cfg.INPUT $filepath\n","            \n","        \n","        for path in cfg.DATASET_PATH:\n","            datasetpath = os.path.join(cfg.DATASET,  path.split('/')[1])\n","            if not os.path.exists(datasetpath):\n","                os.makedirs(datasetpath, exist_ok=True)\n","                ! kaggle datasets download $path -p $datasetpath\n","                filepath = os.path.join(datasetpath, path.split(\"/\")[1]+'.zip')\n","                ! unzip -d $datasetpath $filepath\n","\n","    else:\n","        print('This environment is Kaggle Kernel')\n","\n","        # set dirs\n","        cfg.INPUT = f'../input/{cfg.COMPETITION}'\n","        cfg.EXP = cfg.NAME\n","        cfg.OUTPUT_EXP = cfg.NAME\n","        cfg.SUBMISSION = './'\n","        cfg.DATASET = '../input/'\n","        \n","        cfg.EXP_MODEL = os.path.join(cfg.EXP, 'model')\n","        cfg.EXP_FIG = os.path.join(cfg.EXP, 'fig')\n","        cfg.EXP_PREDS = os.path.join(cfg.EXP, 'preds')\n","\n","        # make dirs\n","        for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n","            os.makedirs(d, exist_ok=True)\n","    return cfg"],"metadata":{"id":"vgJg5DPVpaPd","executionInfo":{"status":"ok","timestamp":1652517506393,"user_tz":-540,"elapsed":549,"user":{"displayName":"クルトン","userId":"17231302919530674825"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n","# This must be done before importing transformers\n","\n","cfg = setup(Config)\n","\n","import shutil\n","from pathlib import Path\n","\n","transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n","\n","convert_file = cfg.OUTPUT_EXP + \"/convert_slow_tokenizer.py\"\n","conversion_path = transformers_path/convert_file.name\n","\n","if conversion_path.exists():\n","    conversion_path.unlink()\n","\n","shutil.copy(convert_file, transformers_path)\n","deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n","\n","for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n","    filepath = deberta_v2_path/filename\n","    \n","    if filepath.exists():\n","        filepath.unlink()\n","\n","    shutil.copy(os.path.join(cfg.OUTPUT_EXP, filename), filepath)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:18:12.909594Z","iopub.execute_input":"2022-05-07T01:18:12.909814Z","iopub.status.idle":"2022-05-07T01:18:12.941347Z","shell.execute_reply.started":"2022-05-07T01:18:12.909788Z","shell.execute_reply":"2022-05-07T01:18:12.940658Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":263},"id":"rGxs9dG0nO6R","executionInfo":{"status":"error","timestamp":1652517647313,"user_tz":-540,"elapsed":9826,"user":{"displayName":"クルトン","userId":"17231302919530674825"}},"outputId":"b7f92f43-644d-4215-a5ce-05cc0e140290"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["This environment is Google Colab\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ededc1a239e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtransformers_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/opt/conda/lib/python3.7/site-packages/transformers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mconvert_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOUTPUT_EXP\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"convert_slow_tokenizer.py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mconversion_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconvert_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'str'"]}]},{"cell_type":"code","source":["%%writefile mlm.py\n","\n","import argparse\n","import os\n","import json\n","from pathlib import Path\n","\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import torch\n","from datasets import load_dataset\n","import tokenizers\n","import transformers\n","from transformers import AutoTokenizer, AutoConfig\n","from transformers import DataCollatorForLanguageModeling, AutoModelForMaskedLM, Trainer\n","from transformers import TrainingArguments\n","from transformers.utils import logging\n","from IPython import embed  # noqa\n","\n","logging.set_verbosity_info()\n","logger = logging.get_logger(__name__)\n","logger.info(\"INFO\")\n","logger.warning(\"WARN\")\n","KAGGLE_ENV = True if 'KAGGLE_URL_BASE' in set(os.environ.keys()) else False\n","\n","\n","print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n","print(f\"transformers.__version__: {transformers.__version__}\")\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# INPUT_DIR = Path('../input/')\n","# if KAGGLE_ENV:\n","#     OUTPUT_DIR = Path('')\n","#     os.environ[\"WANDB_DISABLED\"] = \"true\"\n","# else:\n","#     OUTPUT_DIR = INPUT_DIR\n","\n","\n","def get_patient_notes_not_used_train():\n","\n","    pppm_abstract = pd.read_csv(cfg.DATASET + \"/pppm_abstract/pppm_abstract.csv\")\n","    pppm_abstract.dropna(inplace = True)\n","    pppm_abstract = pppm_abstract.sample(frac=1, random_state=0).reset_index(drop=True)\n","    print(pppm_abstract.shape)\n","\n","    train_pppm_abstract = \\\n","        pppm_abstract.loc[:int(len(pppm_abstract) * 0.7), :].reset_index(drop=True)\n","    valid_pppm_abstract = \\\n","        pppm_abstract.loc[int(len(pppm_abstract) * 0.7):, :].reset_index(drop=True)\n","\n","    print(train_pppm_abstract.shape)\n","    print(valid_pppm_abstract.shape)\n","    return train_pppm_abstract, valid_pppm_abstract\n","\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"])\n","\n","\n","def get_tokenizer(args):\n","    if 'v3' in str(args.model_path):\n","        from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n","        print('DebertaV2TokenizerFast')\n","        tokenizer = DebertaV2TokenizerFast.from_pretrained(INPUT_DIR / args.model_path, trim_offsets=False)\n","    else:\n","        if args.model_name:\n","            print('model_name', args.model_name)\n","            tokenizer = AutoTokenizer.from_pretrained(args.model_name, trim_offsets=False)\n","        else:\n","            print('model_path', args.model_path)\n","            tokenizer = AutoTokenizer.from_pretrained(INPUT_DIR / args.model_path, trim_offsets=False)\n","    return tokenizer\n","\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--model_name\", type=str, default=\"microsoft/deberta-v3-large\", required=False)\n","    parser.add_argument(\"--model_path\", type=str, default=\"\", required=False)\n","    parser.add_argument(\"--seed\", type=int, default=0, required=False)\n","    parser.add_argument('--debug', action='store_true', required=False)\n","    parser.add_argument('--exp_num', type=str, required=True)\n","    parser.add_argument(\"--param_freeze\", action='store_true', required=False)\n","    parser.add_argument(\"--num_train_epochs\", type=int, default=5, required=False)\n","    parser.add_argument(\"--batch_size\", type=int, default=8, required=False)\n","    parser.add_argument(\"--lr\", type=float, default=2e-5, required=False)\n","    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, required=False)\n","    return parser.parse_args()\n","\n","\n","if __name__ == \"__main__\":\n","\n","    args = parse_args()\n","    args.debug = True\n","    train, valid = get_patient_notes_not_used_train()\n","\n","    if args.debug:\n","        train = train.iloc[:10, :]\n","        valid = valid.iloc[:10, :]\n","        args.batch_seize = 1\n","\n","    def get_text(df):\n","        text_list = []\n","        for text in tqdm(df['abstract']):\n","            if len(text) < 30:\n","                pass\n","            else:\n","                text_list.append(text)\n","        return text_list\n","\n","    train_text_list = get_text(train)\n","    valid_text_list = get_text(valid)\n","\n","    mlm_train_json_path = cfg.OUTPUT_EXP / 'train_mlm.json'\n","    mlm_valid_json_path = cfg.OUTPUT_EXP / 'valid_mlm.json'\n","\n","    for json_path, list_ in zip([mlm_train_json_path, mlm_valid_json_path],\n","                                [train_text_list, valid_text_list]):\n","        with open(str(json_path), 'w') as f:\n","            for sentence in list_:\n","                row_json = {'text': sentence}\n","                json.dump(row_json, f)\n","                f.write('\\n')\n","\n","    datasets = load_dataset(\n","        'json',\n","        data_files={'train': str(mlm_train_json_path),\n","                    'valid': str(mlm_valid_json_path)},\n","        )\n","\n","    if mlm_train_json_path.is_file():\n","        mlm_train_json_path.unlink()\n","    if mlm_valid_json_path.is_file():\n","        mlm_valid_json_path.unlink()\n","    print(datasets[\"train\"][:2])\n","\n","    tokenizer = get_tokenizer(args)\n","\n","    tokenized_datasets = datasets.map(\n","        tokenize_function,\n","        batched=True,\n","        num_proc=1,\n","        remove_columns=[\"text\"],\n","        batch_size=args.batch_size)\n","    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n","\n","    if args.model_name:\n","        print('model_name:', args.model_name)\n","        model_name = args.model_name\n","    else:\n","        print('model_path:', args.model_path)\n","        model_name = INPUT_DIR / args.model_path\n","    config = AutoConfig.from_pretrained(model_name, output_hidden_states=True)\n","\n","    if 'v3' in str(model_name):\n","        model = transformers.DebertaV2ForMaskedLM.from_pretrained(INPUT_DIR / model_name, config=config)\n","    else:\n","        model = AutoModelForMaskedLM.from_pretrained(model_name, config=config)\n","\n","    if args.param_freeze:\n","        # if freeze, Write freeze settings here\n","\n","        # deberta-v3-large\n","        # model.deberta.embeddings.requires_grad_(False)\n","        # model.deberta.encoder.layer[:12].requires_grad_(False)\n","\n","        # deberta-large\n","        model.deberta.embeddings.requires_grad_(False)\n","        model.deberta.encoder.layer[:24].requires_grad_(False)\n","\n","        for name, p in model.named_parameters():\n","            print(name, p.requires_grad)\n","\n","    if args.debug:\n","        save_steps = 100\n","        args.num_train_epochs = 1\n","    else:\n","        save_steps = 100000000\n","\n","    training_args = TrainingArguments(\n","        output_dir=\"output-mlm\",\n","        evaluation_strategy=\"epoch\",\n","        learning_rate=args.lr,\n","        weight_decay=0.01,\n","        save_strategy='no',\n","        per_device_train_batch_size=args.batch_size,\n","        num_train_epochs=args.num_train_epochs,\n","        # report_to=\"wandb\",\n","        run_name=f'output-mlm-{args.exp_num}',\n","        # logging_dir='./logs',\n","        lr_scheduler_type='cosine',\n","        warmup_ratio=0.2,\n","        fp16=True,\n","        logging_steps=500,\n","        gradient_accumulation_steps=args.gradient_accumulation_steps\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=tokenized_datasets[\"train\"],\n","        eval_dataset=tokenized_datasets['valid'],\n","        data_collator=data_collator,\n","        # optimizers=(optimizer, scheduler)\n","    )\n","\n","    trainer.train()\n","\n","    if args.model_name == 'microsoft/deberta-xlarge':\n","        model_name = 'deberta-xlarge'\n","    elif args.model_name == 'microsoft/deberta-large':\n","        model_name = 'deberta-large'\n","    elif args.model_name == 'microsoft/deberta-base':\n","        model_name = 'deberta-base'\n","    elif args.model_path == \"../input/deberta-v3-large/deberta-v3-large/\":\n","        model_name = 'deberta-v3-large'\n","    elif args.model_name == 'microsoft/deberta-v2-xlarge':\n","        model_name = 'deberta-v2-xlarge'\n","    trainer.model.save_pretrained(cfg.OUTPUT_EXP + f'/{args.exp_num}_mlm_{model_name}')\n","\n"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T01:18:12.943273Z","iopub.execute_input":"2022-05-07T01:18:12.943677Z","iopub.status.idle":"2022-05-07T01:18:12.955692Z","shell.execute_reply.started":"2022-05-07T01:18:12.943642Z","shell.execute_reply":"2022-05-07T01:18:12.954855Z"},"trusted":true,"id":"rJ5iDhssnO6T","executionInfo":{"status":"aborted","timestamp":1652517525220,"user_tz":-540,"elapsed":5,"user":{"displayName":"クルトン","userId":"17231302919530674825"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python mlm.py --debug --exp_num 0"],"metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:18:12.958488Z","iopub.execute_input":"2022-05-07T01:18:12.959200Z","iopub.status.idle":"2022-05-07T01:18:52.479549Z","shell.execute_reply.started":"2022-05-07T01:18:12.959172Z","shell.execute_reply":"2022-05-07T01:18:52.477939Z"},"trusted":true,"id":"UCb1SODinO6W","executionInfo":{"status":"aborted","timestamp":1652517525221,"user_tz":-540,"elapsed":5,"user":{"displayName":"クルトン","userId":"17231302919530674825"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ls "],"metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:18:52.483853Z","iopub.execute_input":"2022-05-07T01:18:52.484131Z","iopub.status.idle":"2022-05-07T01:18:53.191078Z","shell.execute_reply.started":"2022-05-07T01:18:52.484100Z","shell.execute_reply":"2022-05-07T01:18:53.190263Z"},"trusted":true,"id":"slvTGARgnO6X","outputId":"996f3aec-d6b0-4f70-b144-1c145079b561"},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[0m\u001b[01;34m0_mlm_deberta-v3-large\u001b[0m/  __notebook_source__.ipynb  mlm.py  \u001b[01;34moutput-mlm\u001b[0m/\n","output_type":"stream"}]},{"cell_type":"code","source":[""],"metadata":{"id":"XPu4JsxsnO6X"},"execution_count":null,"outputs":[]}]}